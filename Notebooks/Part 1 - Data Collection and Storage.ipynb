{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is for the data collection Date - 9/04/20\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code cell is for the imports we make two imports only\n",
    "\n",
    "1. praw - for scrapping reddit data\n",
    "2. pandas - for storing data as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import string\n",
    "import datetime as dt\n",
    "from praw.models import MoreComments\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell makes an reddit object to connect to the reddit api and this will be used to search and find the subreddits of our choice. The config dictionary contains the parameters needed to connect to the reddit api\n",
    "\n",
    "1. client_id\n",
    "2. client_secret\n",
    "3. user_agent\n",
    "4. username\n",
    "5. password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'client_id':'paPOXE0wodYqVg',\n",
    "    'client_secret':'e-ciuNWTuEHcO4GY-ZWlovGahT0',\n",
    "    'user_agent':'data_collection',\n",
    "    'user_name':'anuj_2110',\n",
    "    'password':'21101998@anj'}\n",
    "\n",
    "reddit  = praw.Reddit(client_id = config['client_id'],\n",
    "                      client_secret = config['client_secret'],\n",
    "                      user_agent = config['user_agent'],\n",
    "                      user_name = config['user_name'],\n",
    "                      password = config['password'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining variables and functions for data collection purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text): \n",
    "    '''\n",
    "    function for cleaning the text\n",
    "    '''\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)\n",
    "    result = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "    return result.lower()\n",
    "def time_from_timestamp(timestamp):\n",
    "    '''\n",
    "    function for converting the timestamp to time\n",
    "    '''\n",
    "    return dt.datetime.fromtimestamp(timestamp)\n",
    "\n",
    "def retrieve_data(reddit):\n",
    "    '''\n",
    "    This function is for retrieving data from the reddit api,cleaning and storing it\n",
    "    '''\n",
    "    flairs = ['AskIndia','Non-Political','Scheduled','Photography','Coronavirus',\n",
    "          'Science/Technology','Politics','Business/Finance','Policy/Economy','Sports','Food'] # a list for flairs which will be used for searching by flairs\n",
    "    data = {\n",
    "             'id': [],\n",
    "             'flair' : [],\n",
    "             'title' : [],\n",
    "             'body'  : [],\n",
    "             'num_cmnts' : [],\n",
    "             'created': [],\n",
    "             'score': [],\n",
    "             'comments': []} #we will store the data in this dictionary and afterwards use it to store it as pandas dataframe\n",
    "\n",
    "    df_file = \"../reddit_flair_data.csv\" #file for saving data\n",
    "\n",
    "    subred = reddit.subreddit('india') # making the subreddit object for india\n",
    "    \n",
    "    for flair in flairs:\n",
    "        '''\n",
    "        here we parse through each flair and we try to search for subreddits for one flair at a time and store them\n",
    "        '''\n",
    "        subreds = subred.search(flair, limit=500)\n",
    "        for submission in subreds:\n",
    "            data[\"flair\"].append(flair)\n",
    "            data[\"title\"].append(submission.title)\n",
    "            data[\"score\"].append(submission.score)\n",
    "            data[\"id\"].append(submission.id)\n",
    "            data[\"num_cmnts\"].append(submission.num_comments)\n",
    "            data[\"created\"].append(submission.created)\n",
    "            data[\"body\"].append(submission.selftext)\n",
    "            comment = ''\n",
    "            for top_level_comment in submission.comments:\n",
    "                if isinstance(top_level_comment, MoreComments):\n",
    "                    continue\n",
    "                comment = comment + ' ' + top_level_comment.body\n",
    "            data[\"comments\"].append(comment)\n",
    "        print(flair+\" done\")\n",
    "        \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    df['created'] = df['created'].apply(time_from_timestamp)\n",
    "    df['title'] = df['title'].apply(clean_text)\n",
    "    df['body'] =  df['body'].apply(clean_text)\n",
    "    df['comments'] = df['comments'].apply(clean_text)\n",
    "    \n",
    "    df.to_csv(df_file,index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AskIndia done\n",
      "Non-Political done\n",
      "Scheduled done\n",
      "Photography done\n",
      "Coronavirus done\n",
      "Science/Technology done\n",
      "Politics done\n",
      "Business/Finance done\n",
      "Policy/Economy done\n",
      "Sports done\n",
      "Food done\n"
     ]
    }
   ],
   "source": [
    "df = retrieve_data(reddit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
