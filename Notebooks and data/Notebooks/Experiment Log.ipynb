{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Data Collection\n",
    "\n",
    "1. Started with searching for the solution to collect the dataset for this problem. First thought to use beautifulsoup to scrape the pages but with further google search found praw library for python which suited my case.\n",
    "\n",
    "\n",
    "2. Then made an app in reddit to use the praw package in python. Read through the documentation and some blogs based on it. Then collected some data from ''subred.hot'' , ''subed.top api''. But on analysis found that the flairs coming in are highly imbalanced.\n",
    "\n",
    "3. Then searched on internet for possible solution and got to know that we can search by flair type also. So used this for my data collection part. Used ''subred.search(flair)''. Set the limit to 500.\n",
    "\n",
    "\n",
    "4. During the work I found that some blog posts used old api and there are some changes. Like in the comment extraction I got stuck between the versions.\n",
    "\n",
    "\n",
    "5. After getting all my doubts clear I collected the dataset which had Id,title,body,flair,url,num_comments,score and comments of a particular blogpost.\n",
    "\n",
    "\n",
    "6. While collecting and storing the dataset the cleaning of the text is applied. Removed all urls and special characters. Also lowered the case.\n",
    "\n",
    "Blogs and Documentation I followed:\n",
    "  1. https://praw.readthedocs.io/en/latest/\n",
    "  2. https://www.storybench.org/how-to-scrape-reddit-with-python/\n",
    "  3. https://www.pythonforengineers.com/build-a-reddit-bot-part-1/\n",
    "  4. https://www.storybench.org/how-to-scrape-reddit-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Data Analysis\n",
    "\n",
    "1. After collecting the data I searched what analysis can show what features are important.\n",
    "\n",
    "\n",
    "2. On some searching I came to conclusion to see:\n",
    "\n",
    "   2.1 label distribution. This came out to be almost evenly distributed\n",
    "   \n",
    "   2.2 Average sentiment of the posts and comments (combined). Many posts and comments had positive sentiment\n",
    "   \n",
    "   2.3 average word count of each post. \n",
    "   \n",
    "   2.4 average num of comments in each posts.\n",
    "   \n",
    "   2.5 correlation between score,sentiment,word_count,num_comments and flair. This analysis showed that the flair isn't dependent on these features.\n",
    "   \n",
    "   2.6 Word Cloud accompanying each flair in the dataset. This showed that almost all flairs have 5-10 words in common. This can be seen on closely looking at Results.png which contains word cloud.\n",
    "\n",
    "\n",
    "3. During analysis I realized that in cleaning of the text I forgot to remove the urls given in post and comments. So I fixed this issue and collected the dataset again. This was needed as all the flairs' word clouds were showing https as the profound word and it could have harmed the ML pipeline.\n",
    "\n",
    "\n",
    "4. Seeing the quantity of data I tried to collect more samples with the subred.top and other apis but it was causing repitition and also imbalance in the dataset. So I sticked with this approach only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Training ML models\n",
    "\n",
    "1. Used the sklearn library for model building. Started with loading and preprocessing the dataset. Used TfidfVectorizer from sklearn which firstly counts each word occurence and then assign relevance of each word.\n",
    "\n",
    "2. Following are the models trained on the dataset:\n",
    "\n",
    "   2.1 Naive Bayes\n",
    "   \n",
    "   2.2 Linear SVM\n",
    "   \n",
    "   2.3 Decision Trees\n",
    "   \n",
    "   2.4 Logistic Regression\n",
    "   \n",
    "   2.5 Random Forest\n",
    "   \n",
    "   2.6 SVM with RBF kernel\n",
    "   \n",
    "   2.7 Multi Layered Perceptron\n",
    "   \n",
    "   \n",
    "3. The best classifier was the logistic regression classifier with an accuracy of 83.7 saved as (finalmodel2.sav). After training and saving it I tried to tweak other models to see the results but by mistake changed the saved model. But using version control retrieved it and added the code to save only the best model out of the pool. After running and tweaking all the parameters the earlier model stood out the best. \n",
    "\n",
    "\n",
    "4. Tweaked the TfidfVectorizer to extract 5000,6000,7000 and 10000 features from the dataset. Found that extracting 6000 features gave the best results.\n",
    "\n",
    "5. 'alpha': 0.0001, 'loss': 'log', 'max_iter': 300, 'penalty': 'l1'. These parameters I got after training and applying grid search.\n",
    "\n",
    "\n",
    "Blogs I followed:\n",
    "  1. https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
    "  2. https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 Making the web app\n",
    "\n",
    "\n",
    "1. Picked up Django framework for making the web app.\n",
    "\n",
    "\n",
    "2. Django has a nice and clean working approach hence the web app got up and running in quick time. I also had experience in it so it did not required that much work.\n",
    "\n",
    "3. Integrating the model and not affecting the speed of web app much was the major concern. The best strategy was to load everything before hand and just make function calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 Deployment to heroku\n",
    "\n",
    "\n",
    "1. This did took a lot of effort as I never deployed a web app on heroku. Read many articles and after trial and removal finally deployed.\n",
    "\n",
    "Blogs and resources referred:\n",
    "  1. https://simpleisbetterthancomplex.com/tutorial/2016/08/09/how-to-deploy-django-applications-on-heroku.html\n",
    "  2. https://devcenter.heroku.com/articles/deploying-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
